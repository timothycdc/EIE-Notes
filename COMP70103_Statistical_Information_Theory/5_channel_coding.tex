\chapter{Channel Coding}

\section{Noisy Channels}

\subsection{The Big Picture}
\begin{itemize}
    \item Previously, we explored source codes under ideal, noiseless conditions.
    \item However, this assumption is often unrealistic in practical communication settings.
    \item This lecture introduces the concept of \textbf{noisy channels}, where communication occurs in the presence of noise, affecting both transmission and reception.
\end{itemize}

\subsection{Discrete Memoryless Channel (DMC)}
\defb{Discrete Memoryless Channel}{
    A \textbf{discrete memoryless channel (DMC)} consists of:
    \begin{itemize}
        \item \textbf{Input alphabet} \( X \)
        \item \textbf{Output alphabet} \( Y \)
        \item \textbf{Conditional probability distribution} \( p(y|x) \), which represents the probability of receiving output \( y \) given input \( x \).
    \end{itemize}
    For longer messages, we consider the \textbf{extended channel} \( p(y^n|x^n) = \prod_{i=1}^{n} p(y_i|x_i) \), where \( n \) independent transmissions occur in parallel.
}

\subsection{Binary Symmetric Channel (BSC)}
\begin{marginfigure}
    \centering
    \begin{tikzpicture}[->, >=stealth, node distance=1cm]
        % Nodes for input and output states, positioned relatively
        \node (x0) {\( x = 0 \)};
        \node (x1) [below=of x0] {\( x = 1 \)};
        \node (y0) [right=of x0] {\( y = 0 \)};
        \node (y1) [right=of x1] {\( y = 1 \)};

        % Arrows for correct transmission with labels
        \draw[->] (x0) -- (y0) node[midway, above] {};
        \draw[->] (x1) -- (y1) node[midway, above] {};

        % Arrows for flip (error) with labels
        \draw[->] (x0) -- (y1) node[midway, right] {};
        \draw[->] (x1) -- (y0) node[midway, left] {};
    \end{tikzpicture}
    \caption{Binary Symmetric Channel}
\end{marginfigure}

\begin{itemize}
    \item A Binary Symmetric Channel (BSC) is a specific type of DMC with binary input and output alphabets, where each transmitted bit has a probability \( f \) of being flipped.
    \item Transmission probabilities:
          \begin{align*}
              P(y = 0 | x = 0) & = 1 - f, & P(y = 1 | x = 0) & = f,     \\
              P(y = 0 | x = 1) & = f,     & P(y = 1 | x = 1) & = 1 - f.
          \end{align*}
\end{itemize}

\ex{BSC of a Coin Flip}{
    \raggedright
    Consider a BSC with input distribution, \( p(X = 0) = p(X = 1) = 0.5 \), and a flip probability \( f \). \bigskip

    We recall the binary entropy function from Section \ref{sec:entropy_coin}, where \( H_2(f) = -f \log_2(f) - (1 - f) \log_2(1 - f) \) is the entropy of a Bernoulli distribution with parameter \( f \).

    \begin{itemize}
        \item We calculate the mutual information \( I(X; Y) \) between the input \( X \) and output \( Y \).
        \item Since conditional distribution \( p(Y | X) \) is Bernoulli and the channel is symmetric, meaning that 
        \[ H(Y | X = 0) = H(Y | X = 1) = H_2(f) \]
        \item We then have
        \[
            H(Y | X) = H_2(f)
        \]
        \item By symmetry, \( p(Y=0) = p(Y=1) = 0.5  \Rightarrow H(Y) = 1\), thus:
        \begin{align*}
            I(X; Y) &= H(Y) - H(Y | X) \\ &= 1 - H_2(f)
        \end{align*}
    \end{itemize}
    Using these results, the mutual information \( I(X; Y) \) is:
    \[
        I(X; Y) = H(Y) - H(Y | X) = 1 - \big(-f \log_2(f) - (1 - f) \log_2(1 - f)\big)
    \]
}
